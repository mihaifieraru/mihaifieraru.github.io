[{"authors":["admin"],"categories":null,"content":"I am a 7th year PhD student advised by Cristian Sminchisescu.\nMy current interests lie in building algorithms that perceive people in 3d from visual data.\nBetween 2021 and 2023, I was a Research Scientist at Google Research. Before starting my PhD, I received an IMPRS-CS scholarship and fellowship from the Max Planck Institute for Informatics for the duration of my MSc. My thesis there was supervised by Bernt Schiele.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mihaifieraru.github.io/author/mihai-fieraru/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mihai-fieraru/","section":"authors","summary":"I am a 7th year PhD student advised by Cristian Sminchisescu.\nMy current interests lie in building algorithms that perceive people in 3d from visual data.\nBetween 2021 and 2023, I was a Research Scientist at Google Research.","tags":null,"title":"Mihai Fieraru","type":"authors"},{"authors":["Mihai Fieraru","Mihai Zanfir","Elisabeta Oneata","Alin-Ionut Popa","Vlad Olaru","Cristian Sminchisescu"],"categories":null,"content":"","date":1691020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691020800,"objectID":"123f3d80ae27383d3f86db24c0d8b430","permalink":"https://mihaifieraru.github.io/publication/fieraru_2023_tpami/","publishdate":"2023-08-03T00:00:00Z","relpermalink":"/publication/fieraru_2023_tpami/","section":"publication","summary":"Understanding 3d human interactions is fundamental for fine-grained scene analysis and behavioural modeling. However, most of the existing models predict incorrect, lifeless 3d estimates, that miss the subtle human contact aspects--the essence of the event--and are of little use for detailed behavioral understanding. This paper addresses such issues with several contributions: (1) we introduce models for interaction signature estimation (ISP) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged to ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; specifically, we introduce CHI3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing  contact events,  ground truth 3d poses, as well as FlickrCI3D, a dataset of  images, with  processed pairs of people, and  facet-level surface correspondences. Finally, (4) we propose methodology for recovering the ground-truth pose and shape of interacting people in a controlled setup and (5) annotate all 3d interaction motions in CHI3D with textual descriptions. Motion data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is made available for research purposes at \\url{https://ci3d.imar.ro}, together with an evaluation server and a public benchmark.","tags":null,"title":"Reconstructing Three-Dimensional Models of Interacting Humans","type":"publication"},{"authors":["Nikos Kolotouros","Thiemo Alldieck","Andrei Zanfir","Eduard Gabriel Bazavan","Mihai Fieraru","Cristian Sminchisescu"],"categories":null,"content":"","date":1686787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686787200,"objectID":"35f5ea0906551c68a56e1c4d2b096147","permalink":"https://mihaifieraru.github.io/publication/kolotouros_2023_neurips/","publishdate":"2023-06-15T00:00:00Z","relpermalink":"/publication/kolotouros_2023_neurips/","section":"publication","summary":"We present DreamHuman, a method to generate realistic animatable 3D human avatar models solely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than animated 3D human models, and anthropometric consistency for complex structures like people remains a challenge. DreamHuman connects large text-to-image synthesis models, neural radiance fields, and statistical human body models in a novel modeling and optimization framework. This makes it possible to generate dynamic 3D human avatars with high-quality textures and learned, instance-specific, surface deformations. We demonstrate that our method is capable to generate a wide variety of animatable, realistic 3D human models from text. Our 3D models have diverse appearance, clothing, skin tones and body shapes, and significantly outperform both generic text-to-3D approaches and previous text-based 3D avatar generators in visual fidelity.","tags":null,"title":"DreamHuman: Animatable 3D Avatars from Text","type":"publication"},{"authors":["Mihai Fieraru","Mihai Zanfir","Teodor Szente","Eduard Bazavan","Vlad Olaru","Cristian Sminchisescu"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"0cdf348965e1215e3eeff44396b443be","permalink":"https://mihaifieraru.github.io/publication/fieraru_2021_neurips/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/fieraru_2021_neurips/","section":"publication","summary":"The three-dimensional reconstruction of multiple interacting humans given a monocular image is crucial for the general task of scene understanding, as capturing the subtleties of interaction is often the very reason for taking a picture. Current 3D human reconstruction methods either treat each person independently, ignoring most of the context, or reconstruct people jointly, but cannot recover interactions correctly when people are in close proximity. In this work, we introduce \\textbf{REMIPS}, a model for 3D \\underline{Re}construction of \\underline{M}ultiple \\underline{I}nteracting \\underline{P}eople under Weak \\underline{S}upervision. \\textbf{REMIPS} can reconstruct a variable number of people directly from monocular images. At the core of our methodology stands a novel transformer network that combines unordered person tokens (one for each detected human) with positional-encoded tokens from image features patches. We introduce a novel unified model for self- and interpenetration-collisions based on a mesh approximation computed by applying decimation operators. We rely on self-supervised losses for flexibility and generalisation in-the-wild and incorporate self-contact and interaction-contact losses directly into the learning process. With \\textbf{REMIPS}, we report state-of-the-art quantitative results on common benchmarks even in cases where no 3D supervision is used. Additionally, qualitative visual results show that our reconstructions are plausible in terms of pose and shape and coherent for challenging images, collected in-the-wild, where people are often interacting.","tags":null,"title":"REMIPS: Physically Consistent 3D Reconstruction of Multiple Interacting People under Weak Supervision","type":"publication"},{"authors":["Mihai Fieraru","Mihai Zanfir","Silviu-Cristian Pirlea","Vlad Olaru","Cristian Sminchisescu"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"d11adaad4f3b8aae37ceb2bf2882f2df","permalink":"https://mihaifieraru.github.io/publication/fieraru_2021_cvpr/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/fieraru_2021_cvpr/","section":"publication","summary":"I went to the gym today, but how well did I do? And where should I improve? Ah, my back hurts slightly... User engagement can be sustained and injuries avoided by being able to reconstruct 3d human pose, shape, and motion, relate it to good training practices, identify errors, and provide early, real-time feedback. In this paper we introduce the first automatic system, AIFit, that performs 3d human sensing for fitness training. The system can be used at home, outdoors, or at the gym. AIFit is able to reconstruct 3d human pose and motion, reliably segment exercise repetitions, and identify in real-time the deviations between standards learnt from trainers, and the execution of a trainee. As a result, localized, quantitative feedback for correct execution of exercises, reduced risk of injury, and continuous improvement is possible. To support research and evaluation, we introduce the first large scale dataset, Fit3D, containing over 3 million images and corresponding 3d human shape and motion capture ground truth configurations, with over 37 repeated exercises, covering all the major muscle groups, performed by instructors and trainees. Our statistical coach is governed by a global parameter that captures how critical it should be of a trainee’s performance. This is an important aspect that helps adapt to a student’s level of fitness (i.e. beginner vs. advanced vs. expert), or to the expected accuracy of a 3d pose reconstruction method. We show that, for different values of the global parameter, our feedback system based on 3d pose estimates achieves good accuracy compared to the one based on ground-truth motion capture. Our statistical coach offers feedback in natural language, and with spatio-temporal visual grounding.","tags":null,"title":"AIFit: Automatic 3D Human-Interpretable Feedback Models for Fitness Training","type":"publication"},{"authors":["Mihai Fieraru","Mihai Zanfir","Elisabeta Oneata","Alin-Ionut Popa","Vlad Olaru","Cristian Sminchisescu"],"categories":null,"content":"","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"f761abb6250d21d4e92af188989bec35","permalink":"https://mihaifieraru.github.io/publication/fieraru_2021_aaai/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/publication/fieraru_2021_aaai/","section":"publication","summary":"Monocular estimation of three dimensional human self-contact is fundamental for detailed scene analysis including body language understanding and behaviour modeling. Existing 3d reconstruction methods do not focus on body regions in self-contact and consequently recover configurations that are either far from each other or self-intersecting, when they should just touch. This leads to perceptually incorrect estimates and limits impact in those very fine-grained analysis domains where detailed 3d models are expected to play an important role. To address such challenges we detect self-contact and design 3d losses to explicitly enforce it. Specifically, we develop a model for Self-Contact Prediction (SCP), that estimates the body surface signature of self-contact, leveraging the localization of self-contact in the image, during both training and inference. We collect two large datasets to support learning and evaluation: (1) HumanSC3D, an accurate 3d motion capture repository containing 1,032 sequences with 5,058 contact events and 1,246,487 ground truth 3d poses synchronized with images collected from multiple views, and (2) FlickrSC3D, a repository of 3,969 images, containing 25,297 surface-to-surface correspondences with annotated image spatial support. We also illustrate how more expressive 3d reconstructions can be recovered under self-contact signature constraints and present monocular detection of face-touch as one of the multiple applications made possible by more accurate self-contact models.","tags":null,"title":"Learning Complex 3D Human Self-Contact","type":"publication"},{"authors":["Mihai Fieraru","Mihai Zanfir","Elisabeta Oneata","Alin-Ionut Popa","Vlad Olaru","Cristian Sminchisescu"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"88693a125c75426f656d3bf26c19f9ce","permalink":"https://mihaifieraru.github.io/publication/fieraru_2020_cvpr/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/fieraru_2020_cvpr/","section":"publication","summary":"Understanding 3d human interactions is fundamental for fine grained scene analysis and behavioural modeling. However, most of the existing models focus on analyzing a single person in isolation, and those who process several people focus largely on resolving multi-person data association, rather than inferring interactions. This may lead to incorrect, lifeless 3d estimates, that miss the subtle human contact aspects--the essence of the event--and are of little use for detailed behavioral understanding. This paper addresses such issues and makes several contributions: (1) we introduce models for interaction signature estimation (ISP) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged in order to produce augmented losses that ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; specifically, we introduce CHI3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing 2,525 contact events, 728,664 ground truth 3d poses, as well as FlickrCI3D, a dataset of 11,216 images, with 14,081 processed pairs of people, and 81,233 facet-level surface correspondences within 138,213 selected contact regions. Finally, (4) we present models and baselines to illustrate how contact estimation supports meaningful 3d reconstruction where essential interactions are captured. Models and data are made available for research purposes at http://vision.imar.ro/ci3d.","tags":null,"title":"Three-Dimensional Reconstruction of Human Interactions","type":"publication"},{"authors":["Mihai Fieraru","Anna Khoreva","Leonid Pischulin","Bernt Schiele"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9dc7fae5bd075d7fa5908be7443b0a15","permalink":"https://mihaifieraru.github.io/publication/fieraru_2018_cvpr_workshops/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/fieraru_2018_cvpr_workshops/","section":"publication","summary":"Multi-person pose estimation in images and videos is an important yet challenging task with many applications. Despite the large improvements in human pose estimation enabled by the development of convolutional neural networks, there still exist a lot of difficult cases where even the state-of-the-art models fail to correctly localize all body joints. This motivates the need for an additional refinement step that addresses these challenging cases and can be easily applied on top of any existing method. In this work, we introduce a pose refinement network (PoseRefiner) which takes as input both the image and a given pose estimate and learns to directly predict a refined pose by jointly reasoning about the input-output space. In order for the network to learn to refine incorrect body joint predictions, we employ a novel data augmentation scheme for training, where we model “hard“ human pose cases. We evaluate our approach on four popular large-scale pose estimation benchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack Pose Estimation, and PoseTrack Pose Tracking, and report systematic improvement over the state of the art.","tags":null,"title":"Learning to Refine Human Pose Estimation","type":"publication"},{"authors":["Mihai Fieraru"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"14c17958bd806df1f766296d966a2d2a","permalink":"https://mihaifieraru.github.io/publication/mihaifierarumscthesis/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/mihaifierarumscthesis/","section":"publication","summary":"This thesis addresses the multi-person tracking task with two types of representation: body pose and segmentation mask. We explore these scenarios in the semi-supervised setting, where one available annotation is available per person during test time. More complex representations of people (segmentation mask and body pose) can provide richer understanding of visual scenes, and methods that leverage supervision during test time should be developed for the cases when supervision is available. We propose HumanMaskTracker for the task of semi-supervised multi-person mask tracking. Our approach builds on recent techniques proposed for the task of video object segmentation. These include the mask refinement approach, training with syn- thetic data, fine-tuning per object and leveraging optical flow. In addition, we propose leveraging instance semantic segmentation proposals to give the tracker a better notion about the human class. Moreover, we propose modeling people occlusions inside the data synthesis process to make the tracker more robust to the challenges of occlusion and disocclusion. For the task of semi-supervised multi-person pose tracking, we propose the method HumanPoseTracker. We show that the task of multi-person pose tracking can benefit significantly from using one pose supervision per track during test time. Fine-tuning per object and leveraging optical flow, techniques proposed for the task of video object segmentation, prove to be highly effective for supervised pose tracking as well. Also, we propose a technique to remove false positive joint detections and develop tracking stopping criteria. A promising application of our work is presented by extending the method to generate dense from sparse annotations in videos.","tags":null,"title":"Learning to Track Humans in Videos","type":"publication"}]